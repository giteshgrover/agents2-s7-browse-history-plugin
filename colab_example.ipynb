{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load FAISS Index in Google Colab\n",
        "\n",
        "This notebook demonstrates how to load and use the FAISS index exported from the Chrome extension backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install faiss-cpu sentence-transformers numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import zipfile\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the exported zip file\n",
        "uploaded = files.upload()\n",
        "zip_filename = list(uploaded.keys())[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('faiss_index')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FAISS index and metadata\n",
        "index = faiss.read_index('faiss_index/index.faiss')\n",
        "with open('faiss_index/metadata.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "\n",
        "print(f\"Index loaded: {index.ntotal} vectors\")\n",
        "print(f\"Metadata: {len(metadata)} entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the embedding model (same as used for indexing)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search function\n",
        "def search(query, top_k=5):\n",
        "    # Encode query\n",
        "    query_embedding = model.encode([query])\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "    \n",
        "    # Search\n",
        "    k = min(top_k, index.ntotal)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    \n",
        "    # Get results\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if idx < len(metadata):\n",
        "            result = metadata[idx].copy()\n",
        "            result['distance'] = float(distances[0][i])\n",
        "            results.append(result)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example search\n",
        "query = \"your search query here\"\n",
        "results = search(query, top_k=5)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\nResult {i}:\")\n",
        "    print(f\"Title: {result['title']}\")\n",
        "    print(f\"URL: {result['url']}\")\n",
        "    print(f\"Distance: {result['distance']:.4f}\")\n",
        "    print(f\"Chunk: {result['chunk_text'][:200]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
